{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8: The inverse Ising problem\n",
    "\n",
    "**Due Friday, June 3**  \n",
    "\n",
    "*Enter your name here*  \n",
    "\n",
    "### Homework checklist\n",
    "\n",
    "Before submitting, make sure that you\n",
    "\n",
    "- Fill in your name in the space above\n",
    "- Cite any resources that you used while working on this homework\n",
    "- 1.a. Fill in the code to define the energy function  \n",
    "- 1.b. Fill in the code to compute the partition function  \n",
    "- 1.c. Fill in the code to compute the derivative of minus the log-likelihood function  \n",
    "- 2.a. Read in the simulation data    \n",
    "- 2.b. Fill in the code for the steepest descent algorithm  \n",
    "- 3.a. Run the steepest descent algorithm\n",
    "- 3.b. Analyze your results, and discuss how you might verify them    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Previously, we studied the [Ising model](https://en.wikipedia.org/wiki/Ising_model), a simple model from statistical physics. The Ising model was originally applied to study ferromagnetism, but it has since been generalized and applied to understand a wide variety of physical and biological phenomena.\n",
    "\n",
    "In our work on the Ising model, we wanted to understand how changing the parameters of the model affects behavior. In this homework problem, we will consider going in the opposite direction: given that we observe the behavior of an Ising system, how can we infer its parameters? In general, this kind of problem is called an [inverse problem](https://en.wikipedia.org/wiki/Inverse_problem). This problem specifically is referred to as the inverse Ising problem.\n",
    "\n",
    "Inverse problems are very common in practical contexts, but they are also of great theoretical interest. See [here](https://en.wikipedia.org/wiki/Hearing_the_shape_of_a_drum) for a provocative example of a physics-inspired inverse problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Coupled spins\n",
    "\n",
    "For this exercise we'll consider a very simple Ising system: two spins $\\underline{\\sigma}=\\{\\sigma_1, \\sigma_2\\}$, $\\sigma_i\\in\\{\\pm 1\\}$ with a coupling constant $J$. We assume that the energy of interaction between each spin and an external magnetic field is $h$. The energy function is then\n",
    "\n",
    "$$\n",
    "E(\\underline{\\sigma}) = -h\\sigma_1 -h\\sigma_2 -J\\sigma_1 \\sigma_2\\,.\n",
    "$$\n",
    "\n",
    "I've chosen some values for $h$ and $J$, gotten a set of sample configurations using Markov chain Monte Carlo, and saved them in a data file `configurations.csv`. We'll read in this data and analyze it, attempting to find back the original $h$ and $J$. In this section, we'll write a set of helper functions that we'll need to perform maximum likelihood inference.\n",
    "\n",
    "### 1.a. Code the energy function\n",
    "\n",
    "Fill in the code below to define a function, `compute_E`, that computes the energy of a configuration. Assume that the input `sigma` is an array of length two whose first element is $\\sigma_1$ and whose second element is $\\sigma_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Write the function to compute the energy\n",
    "\n",
    "def compute_E(sigma, h, J):\n",
    "    \"\"\" \n",
    "    Returns the energy of a spin configuration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Evaluate the energy and return\n",
    "    \n",
    "    return  # FILL THIS IN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b. Code the partition function\n",
    "\n",
    "In order to compute the probabilities of different configurations, we need to evaluate\n",
    "\n",
    "$$\n",
    "P(\\underline{\\sigma}) = \\frac{e^{-E(\\underline{\\sigma})/T}}{Z}\\,. \n",
    "$$\n",
    "\n",
    "That means that we need to compute the partition function,\n",
    "\n",
    "$$\n",
    "Z = \\sum_{\\underline{\\sigma}} e^{-E(\\underline{\\sigma})/T}\\,.\n",
    "$$\n",
    "\n",
    "Fill in the code below to compute the partition function for a given set of $h$ and $J$ parameters. For simplicity, we'll normalize $T=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the function to compute the energy\n",
    "\n",
    "def compute_Z(h, J):\n",
    "    \"\"\" \n",
    "    Returns the partition function.\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = 0 # initialize\n",
    "    \n",
    "    for sigma1 in [-1, 1]:\n",
    "        for sigma2 in [-1, 1]:\n",
    "            Z += # FILL THIS IN (use compute_E defined above)\n",
    "    \n",
    "    # Return the result\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c. Compute the derivative of the log-likelihood\n",
    "\n",
    "As we have seen, the likelihood is defined as the probability of observing that data given a set of parameters\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\{\\underline{\\sigma}^k\\} | \\{h, J\\}) = \\prod_{k=1}^{N} P_{h,J}(\\underline{\\sigma}^k)\\,.\n",
    "$$\n",
    "\n",
    "Here $\\underline{\\sigma}^k$, $k=1, \\ldots, N$ represent the $N$ configurations of the system that we've observed, and $P_{h,J}(\\underline{\\sigma})$ represents the probability of configuration $\\underline{\\sigma}$ in the Ising model with parameters $h$ and $J$. \n",
    "\n",
    "Below, we will write code to find the values of $h$ and $J$ that maximize the likelihood of the data. In order to do this, we need to compute the derivative of the likelihood with respect to $h$ and $J$ so that we can use a gradient-based optimization algorithm to maximize it. \n",
    "\n",
    "This is easier numerically if we compute the gradient of minus the logarithm of the likelihood, $\\ell$, instead. To do that, we take minus the logarithm of the above expression and compute derivatives with respect to $h$ and $J$. We will also divide by the number of observations $N$ for numerical stability. You can easily verify that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\frac{d \\ell}{N d h} &= -\\frac{1}{N}\\sum_{i=k}^{N} \\left(\\sigma^k_1 + \\sigma^k_2\\right) + \\left\\langle \\sigma_1 + \\sigma_2\\right\\rangle_{h, J}\\,,\\\\\n",
    "-\\frac{d \\ell}{N d J} &= -\\frac{1}{N}\\sum_{i=k}^{N} \\left(\\sigma^k_1 \\times \\sigma^k_2\\right) + \\left\\langle \\sigma_1 \\times \\sigma_2\\right\\rangle_{h, J}\\,.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To be clear, $\\sigma^k_i$ represents the value of the $i$th spin in the $k$th configuration in the data. The average $\\left\\langle \\cdot \\right \\rangle_{h, J}$ is an average over the Gibbs distribution for the Ising model with parameters $h$ and $J$,\n",
    "\n",
    "$$\n",
    "\\left\\langle f(\\underline{\\sigma}) \\right\\rangle = \\sum_{\\underline{\\sigma}} f(\\underline{\\sigma}) P(\\underline{\\sigma})\\,.\n",
    "$$\n",
    "\n",
    "It is now straightforward to compute these quantities using the functions that we've derived above. Fill in the code below to compute the gradient of minus the log-likelihood. We'll take `sigmas` to be a vector that holds the configuration data from the simulation. Each row represents a configuraton, i.e, a set of two Ising spin values. Similarly, `hJ` is also a vector whose first entry is the value of $h$ and whose second entry is the value of $J$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(hJ, sigmas):\n",
    "    \"\"\" \n",
    "    Compute and return the gradient of minus the log-likelihood \n",
    "    with respect to the parameters h, J \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the gradient array\n",
    "    \n",
    "    grad = np.zeros(2)\n",
    "    \n",
    "    # Pre-compute the partition function\n",
    "    \n",
    "    Z = compute_Z(hJ[0], hJ[1])\n",
    "    \n",
    "    \n",
    "    # The first entry is -dl/dh\n",
    "    # First, get the contribution from the data\n",
    "    \n",
    "    for k in range(len(sigmas)):\n",
    "        grad[0] +=  # FILL THIS IN (for a hint, see below)\n",
    "    \n",
    "    # Second, get the contribution from the average over the model\n",
    "    \n",
    "    for sigma1 in [-1, 1]:\n",
    "        for sigma2 in [-1, 1]:\n",
    "            grad[0] += (sigma1 + sigma2) * np.exp(-compute_E([sigma1, sigma2], hJ[0], hJ[1])) / Z\n",
    "            \n",
    "    \n",
    "    # The second entry is -dl/dJ\n",
    "    # First, get the contribution from the data\n",
    "    \n",
    "    for k in range(len(sigmas)):\n",
    "        grad[1] += -sigmas[k][0] * sigmas[k][1] / float(len(sigmas))\n",
    "        \n",
    "    # Second, get the contribution from the average over the model\n",
    "    \n",
    "    for sigma1 in [-1, 1]:\n",
    "        for sigma2 in [-1, 1]:\n",
    "            grad[1] +=  # FILL THIS IN (for a hint, see above)\n",
    "    \n",
    "    # Return the result\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read in the data and code the optimization algorithm.\n",
    "\n",
    "As in past exercises, we'll use the steepest descent algorithm to find the parameters $h$ and $J$ that maximize the likelihood of the data. But first, we need to read in the data itself. It's stored in a `.csv` file, `configurations.csv`. You might want to look at the file directly. The first line of the file tells us what the values in each column mean. Each subsequent row is an observation, with the values in each columns separated by a comma. This is why this is called a `.csv` file: it stores **c**omma **s**eparated **v**alues.\n",
    "\n",
    "### 2.a. Read in the data\n",
    "\n",
    "Execute the code block below to read in the data and compute some summary statistics. We'll use a builtin function from `numpy`, `loadtxt` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "\n",
    "sigmas = np.loadtxt('configurations.csv', delimiter=',')\n",
    "\n",
    "\n",
    "# Compute summary statistics\n",
    "\n",
    "print('The average value of the spin at site 1 is %.3f' % np.mean(sigmas.T[0]))\n",
    "print('The average value of the spin at site 2 is %.3f' % np.mean(sigmas.T[1]))\n",
    "print('The average value of the sum of the spins at both sites is %.3f' % np.mean(sigmas))\n",
    "print('The average value of the product of the spins is %.3f' % np.mean(sigmas.T[0]*sigmas.T[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b. Code the steepest descent algorithm\n",
    "\n",
    "Fill in the code below to define a function `steepest_descent` that uses the steepest descent algorithm to find the minimum of minus the log-likelihood. The input to the function is the derivative function, `df`, the starting value, `x0`, and the data `sigmas`. In general the inverse Ising problem can be challenging, but this particular case should converge readily even if we choose a constant step size $t = 0.1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(df, x0, sigmas):\n",
    "    \"\"\"\n",
    "    Run the steepest descent algorithm to find the minimum of the function whose gradient is df.\n",
    "    The starting value for the function is x0.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the system\n",
    "    \n",
    "    epsilon  = 0.0001  # Stopping condition -- end when |df/dx| < epsilon \n",
    "    max_iter = 100     # Maximum number of iterations\n",
    "    it       = 0       # Current iteration\n",
    "    \n",
    "    x    = x0     # Current parameter value\n",
    "    dfdx =  # FILL THIS IN, Initial value of the derivative\n",
    "    \n",
    "    # Report status\n",
    "    print('iter\\tx\\tdf/dx')\n",
    "    \n",
    "    \n",
    "    # Execute the steepest descent algorithm\n",
    "    \n",
    "    while np.sum(np.fabs(dfdx))>=epsilon and it<max_iter:\n",
    "    \n",
    "        # Report status\n",
    "        print('{}\\t{}\\t{}'.format(it, x, dfdx))\n",
    "\n",
    "        # Choose the step direction\n",
    "        s =  # FILL THIS IN\n",
    "\n",
    "        # Choose how far to step in that direction\n",
    "        t  = 0.1\n",
    "\n",
    "        # Update the parameters\n",
    "        x =  # FILL THIS IN\n",
    "\n",
    "        # Update the derivative\n",
    "        dfdx =  # FILL THIS IN\n",
    "\n",
    "        # Update the iteration counter\n",
    "        it += 1\n",
    "        \n",
    "\n",
    "    # Return the result\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Find the parameters\n",
    "\n",
    "Now that we've defined the steepest descent algorithm, we can now run it to find the underlying values of $h$ and $J$.\n",
    "\n",
    "### 3.a. Run the steepest descent algorithm\n",
    "\n",
    "Fill in the code to run the steepest descent algorithm. We'll start at an initial guess of $h = J = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting position\n",
    "\n",
    "x0 =  # FILL THIS IN\n",
    "\n",
    "\n",
    "# Run steepest descent\n",
    "\n",
    "x_min =  # FILL THIS IN, use your steepest_descent function to find the minimum\n",
    "\n",
    "\n",
    "# Print the results\n",
    "\n",
    "print('Found the minimum at r = {}'.format(x_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.b. Analyze the results\n",
    "\n",
    "Consider the output of the steepest descent algorithm above. Did it converge smoothly? Do the results appear to make sense? How could we check to see whether or not the parameters appear to be reasonable? You don't need to perform a calculation, just to describe what you might consider. Fill in your response in the Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILL THIS IN**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
